<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/PublicPersonalKnowledgeBase/favicon.ico"/><title>pytorch</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="pytorch"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://petrovs12.github.io/PublicPersonalKnowledgeBase/notes/awidUgzHuMsvTiKstZ28g/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="1/10/2022"/><meta property="article:modified_time" content="1/13/2022"/><link rel="canonical" href="https://petrovs12.github.io/PublicPersonalKnowledgeBase/notes/awidUgzHuMsvTiKstZ28g/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/PublicPersonalKnowledgeBase/_next/static/css/7719b08cc3352912.css" as="style"/><link rel="stylesheet" href="/PublicPersonalKnowledgeBase/_next/static/css/7719b08cc3352912.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/PublicPersonalKnowledgeBase/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/webpack-b320c149a25b986a.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/main-8915ee191b7710e8.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/pages/_app-bf98b80149ea852b.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/d47oHCd-7QAOR5IvSuJke/_buildManifest.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/d47oHCd-7QAOR5IvSuJke/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="pytorch">pytorch<a aria-hidden="true" class="anchor-heading icon-link" href="#pytorch"></a></h1>
<p><a href="https://pytorch.org/">Website</a></p>
<h1 id="multiple-submodules">Multiple submodules<a aria-hidden="true" class="anchor-heading icon-link" href="#multiple-submodules"></a></h1>
<h2 id="dataloader">DataLoader<a aria-hidden="true" class="anchor-heading icon-link" href="#dataloader"></a></h2>
<p>It's a iterator helper that would deal w/</p>
<h1 id="neural-nets-forward-and-backwards">Neural Nets, Forward and Backwards<a aria-hidden="true" class="anchor-heading icon-link" href="#neural-nets-forward-and-backwards"></a></h1>
<p> To create a neural net, make a class that inherits from <code>nn.Module</code>.
It needs to implement <code>__init__</code> and <code>forward</code> methods.</p>
<pre class="language-{python}"><code class="language-{python}">class DenseNet(nn.Module):
   def __init__():
       self.input = nn.Flatten()
       self.linear_relu_stack=nn.Sequential(
           nn.Linear(28*25, 256),
           nn.ReLU(),
           nn.Linear(256, 128),
           nn.ReLU(),
           nn.Linear(128, 10)
       )
   def forward(self,x):
       x = self.input(x)
       logits = self.linear_relu_stack(x)
       return logits

model = DenseNet().to(device)# send the model to execute on the gpu/cpu


</code></pre>
<p> So that's how we encode a a given computational tree.
Probably there are various simplifications/specializations of the above. Is</p>
<h2 id="parameter-fittingoptimization-and-backward-pass">Parameter fitting/optimization and backward pass<a aria-hidden="true" class="anchor-heading icon-link" href="#parameter-fittingoptimization-and-backward-pass"></a></h2>
<p>After we have defined the 'evaluation' part, next we want to see how to fit parameters to the data.</p>
<p>Define 'optimizer' object and the loss funciton object
<strong>NB</strong> <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">#NB (Private)</a> optimizer needs to be supplied the model.parameters() as an argument in order to know what it's optimizing.</p>
<p>If you think about a function instead, the optimizer
would need to know the function + it's parameters, so it makes sense.</p>
<pre class="language-{python}"><code class="language-{python}">loss_fn = nn.CrossEntropyLoss()
!!!
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
</code></pre>
<h1 id="training-loop">Training loop<a aria-hidden="true" class="anchor-heading icon-link" href="#training-loop"></a></h1>
<p>We have to define the training loop by ourselves.</p>
<p>The backpropagation is done via the following opaque at first look pattern:</p>
<pre class="language-{python}"><code class="language-{python}">optimizer.zero_grad() # nullify previous gradient data
loss.backward() #compute gradient (!!!)
optimizer.step() #make step w/ the optimizer
</code></pre>
<p>Overall  Ifind the above pattern confusing, as there is quite a bit of state manipulation there.
In particular, the signatures of functions there don't tell you about what's happening.</p>
<p> Nevertheless, it's short and only 'strange' part is</p>
<p><code>loss.backward()</code>. Why is this actinf on the model parameters? no idea.</p>
<p>from <a href="http://seba1511.net/tutorials/beginner/blitz/neural_networks_tutorial.html#:~:text=Loss%20Function,-A%20loss%20function&#x26;text=MSELoss%20which%20computes%20the%20mean,the%20input%20and%20the%20target.&#x26;text=So%2C%20when%20we%20call%20loss,Variable%20accumulated%20with%20the%20gradient.">here</a></p>
<p><strong>So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.</strong></p>
<p> Ok, so that makes sense. Every Variable has a .grad Variable, which is acted upon by <code>loss.backward()</code>.</p>
<pre><code>
def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    # I guess this tells the model that
    # paramteres are free to update
    
    model.train()
    # for each batch of samples
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation and parameter update
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Occasional progress report (like callback)
        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

</code></pre>
<h2 id="what-does-modeltrain-do"><a href="https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch">What Does model.train() do?</a><a aria-hidden="true" class="anchor-heading icon-link" href="#what-does-modeltrain-do"></a></h2>
<p>It sets the model so layers that behave differently during train and test set.</p>
<p><code>model.train()</code> and <code>model.eval()</code> basically only care about dropout and batch normalization layers atm.</p>
<pre class="language-{python}"><code class="language-{python}">data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
</code></pre>
<h1 id="lossbackward"><a href="https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944">loss.backward()</a><a aria-hidden="true" class="anchor-heading icon-link" href="#lossbackward"></a></h1>
<pre><code>loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:
</code></pre>
<p><img src="https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2"></p>
<pre class="language-{python}"><code class="language-{python}">x.grad += dloss/dx

</code></pre>
<p>So because of that we have to 0 the grad beforehand</p>
<h1 id="grad-mode">Grad Mode<a aria-hidden="true" class="anchor-heading icon-link" href="#grad-mode"></a></h1>
<p>(Enable grad)[https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html# enable_grad]</p>
<h2 id="torchno_grad">Torch.no_grad<a aria-hidden="true" class="anchor-heading icon-link" href="#torchno_grad"></a></h2>
<p>Context manager, disabling (thread-wise) grad calculations. Use when evaluating model.</p>
<h2 id="aitem">a.item()<a aria-hidden="true" class="anchor-heading icon-link" href="#aitem"></a></h2>
<p>if object is number/1 element tensor.
use a.tolist() otherwise.</p>
<h1 id="differentiable">Differentiable<a aria-hidden="true" class="anchor-heading icon-link" href="#differentiable"></a></h1>
<p>Some ops in torch are differentiable, some are not.</p>
<h1 id="tensors-and-differeentiataion">Tensors and differeentiataion<a aria-hidden="true" class="anchor-heading icon-link" href="#tensors-and-differeentiataion"></a></h1>
<p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">'Autograd' is some engine,running in the background? Idk</a></p>
<h1 id="resources">resources<a aria-hidden="true" class="anchor-heading icon-link" href="#resources"></a></h1>
<p><a href="http://blog.ezyang.com/2019/05/pytorch-internals/">Pytorch Internals Blog Post</a></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1">Deep Neural Networks</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/K93cKDmInFmPJGz1VVNTF">Differentiable and Probabalistic Programming</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#multiple-submodules" title="Multiple submodules">Multiple submodules</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dataloader" title="DataLoader">DataLoader</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#neural-nets-forward-and-backwards" title="Neural Nets, Forward and Backwards">Neural Nets, Forward and Backwards</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#parameter-fittingoptimization-and-backward-pass" title="Parameter fitting/optimization and backward pass">Parameter fitting/optimization and backward pass</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#training-loop" title="Training loop">Training loop</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#what-does-modeltrain-do" title="What Does model.train() do?">What Does model.train() do?</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lossbackward" title="loss.backward()">loss.backward()</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#grad-mode" title="Grad Mode">Grad Mode</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#torchno_grad" title="Torch.no_grad">Torch.no_grad</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#aitem" title="a.item()">a.item()</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#differentiable" title="Differentiable">Differentiable</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tensors-and-differeentiataion" title="Tensors and differeentiataion">Tensors and differeentiataion</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#resources" title="resources">resources</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"awidUgzHuMsvTiKstZ28g","title":"pytorch","desc":"","updated":1642039817519,"created":1641834743300,"custom":{},"fname":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch","type":"note","vault":{"fsPath":"vault"},"contentHash":"b1dc7c6de18892ea9cb0fbd557a44889","links":[{"type":"wiki","from":{"fname":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch","id":"awidUgzHuMsvTiKstZ28g","vaultName":"vault"},"value":"tags.NB","alias":"#NB","position":{"start":{"line":45,"column":8,"offset":1124},"end":{"line":45,"column":11,"offset":1127},"indent":[]},"xvault":false,"to":{"fname":"tags.NB"}},{"from":{"fname":"science.stats.Deep Neural Networks","id":"1oUqI7ql4EuPu9Ml94Xe1","vaultName":"vault"},"type":"backlink","position":{"start":{"line":3,"column":1,"offset":82},"end":{"line":3,"column":78,"offset":159},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"},{"from":{"fname":"science.stats.Deep Neural Networks","id":"1oUqI7ql4EuPu9Ml94Xe1","vaultName":"vault"},"type":"backlink","position":{"start":{"line":115,"column":2,"offset":2919},"end":{"line":115,"column":79,"offset":2996},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"},{"from":{"fname":"science.CS.theory.Code Transformations.Differentiable and Probabalistic Programming","id":"K93cKDmInFmPJGz1VVNTF","vaultName":"vault"},"type":"backlink","position":{"start":{"line":4,"column":1,"offset":83},"end":{"line":4,"column":78,"offset":160},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"}],"anchors":{"multiple-submodules":{"type":"header","text":"Multiple submodules","value":"multiple-submodules","line":10,"column":0,"depth":1},"dataloader":{"type":"header","text":"DataLoader","value":"dataloader","line":12,"column":0,"depth":2},"neural-nets-forward-and-backwards":{"type":"header","text":"Neural Nets, Forward and Backwards","value":"neural-nets-forward-and-backwards","line":17,"column":0,"depth":1},"parameter-fittingoptimization-and-backward-pass":{"type":"header","text":"Parameter fitting/optimization and backward pass","value":"parameter-fittingoptimization-and-backward-pass","line":46,"column":0,"depth":2},"training-loop":{"type":"header","text":"Training loop","value":"training-loop","line":62,"column":0,"depth":1},"what-does-modeltrain-do":{"type":"header","text":"What Does model.train() do?","value":"what-does-modeltrain-do","line":116,"column":0,"depth":2},"lossbackward":{"type":"header","text":"loss.backward()","value":"lossbackward","line":128,"column":0,"depth":1},"grad-mode":{"type":"header","text":"Grad Mode","value":"grad-mode","line":144,"column":0,"depth":1},"torchno_grad":{"type":"header","text":"Torch.no_grad","value":"torchno_grad","line":148,"column":0,"depth":2},"aitem":{"type":"header","text":"a.item()","value":"aitem","line":152,"column":0,"depth":2},"differentiable":{"type":"header","text":"Differentiable","value":"differentiable","line":156,"column":0,"depth":1},"tensors-and-differeentiataion":{"type":"header","text":"Tensors and differeentiataion","value":"tensors-and-differeentiataion","line":159,"column":0,"depth":1},"resources":{"type":"header","text":"resources","value":"resources","line":164,"column":0,"depth":1}},"children":[],"parent":"qnsew4i2mrvs1xsb4f2zktp","data":{}},"body":"\u003ch1 id=\"pytorch\"\u003epytorch\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pytorch\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pytorch.org/\"\u003eWebsite\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"multiple-submodules\"\u003eMultiple submodules\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#multiple-submodules\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"dataloader\"\u003eDataLoader\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dataloader\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIt's a iterator helper that would deal w/\u003c/p\u003e\n\u003ch1 id=\"neural-nets-forward-and-backwards\"\u003eNeural Nets, Forward and Backwards\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-nets-forward-and-backwards\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e To create a neural net, make a class that inherits from \u003ccode\u003enn.Module\u003c/code\u003e.\nIt needs to implement \u003ccode\u003e__init__\u003c/code\u003e and \u003ccode\u003eforward\u003c/code\u003e methods.\u003c/p\u003e\n\u003cpre class=\"language-{python}\"\u003e\u003ccode class=\"language-{python}\"\u003eclass DenseNet(nn.Module):\n   def __init__():\n       self.input = nn.Flatten()\n       self.linear_relu_stack=nn.Sequential(\n           nn.Linear(28*25, 256),\n           nn.ReLU(),\n           nn.Linear(256, 128),\n           nn.ReLU(),\n           nn.Linear(128, 10)\n       )\n   def forward(self,x):\n       x = self.input(x)\n       logits = self.linear_relu_stack(x)\n       return logits\n\nmodel = DenseNet().to(device)# send the model to execute on the gpu/cpu\n\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e So that's how we encode a a given computational tree.\nProbably there are various simplifications/specializations of the above. Is\u003c/p\u003e\n\u003ch2 id=\"parameter-fittingoptimization-and-backward-pass\"\u003eParameter fitting/optimization and backward pass\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#parameter-fittingoptimization-and-backward-pass\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAfter we have defined the 'evaluation' part, next we want to see how to fit parameters to the data.\u003c/p\u003e\n\u003cp\u003eDefine 'optimizer' object and the loss funciton object\n\u003cstrong\u003eNB\u003c/strong\u003e \u003ca title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\"\u003e#NB (Private)\u003c/a\u003e optimizer needs to be supplied the model.parameters() as an argument in order to know what it's optimizing.\u003c/p\u003e\n\u003cp\u003eIf you think about a function instead, the optimizer\nwould need to know the function + it's parameters, so it makes sense.\u003c/p\u003e\n\u003cpre class=\"language-{python}\"\u003e\u003ccode class=\"language-{python}\"\u003eloss_fn = nn.CrossEntropyLoss()\n!!!\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"training-loop\"\u003eTraining loop\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#training-loop\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eWe have to define the training loop by ourselves.\u003c/p\u003e\n\u003cp\u003eThe backpropagation is done via the following opaque at first look pattern:\u003c/p\u003e\n\u003cpre class=\"language-{python}\"\u003e\u003ccode class=\"language-{python}\"\u003eoptimizer.zero_grad() # nullify previous gradient data\nloss.backward() #compute gradient (!!!)\noptimizer.step() #make step w/ the optimizer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOverall  Ifind the above pattern confusing, as there is quite a bit of state manipulation there.\nIn particular, the signatures of functions there don't tell you about what's happening.\u003c/p\u003e\n\u003cp\u003e Nevertheless, it's short and only 'strange' part is\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eloss.backward()\u003c/code\u003e. Why is this actinf on the model parameters? no idea.\u003c/p\u003e\n\u003cp\u003efrom \u003ca href=\"http://seba1511.net/tutorials/beginner/blitz/neural_networks_tutorial.html#:~:text=Loss%20Function,-A%20loss%20function\u0026#x26;text=MSELoss%20which%20computes%20the%20mean,the%20input%20and%20the%20target.\u0026#x26;text=So%2C%20when%20we%20call%20loss,Variable%20accumulated%20with%20the%20gradient.\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSo, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e Ok, so that makes sense. Every Variable has a .grad Variable, which is acted upon by \u003ccode\u003eloss.backward()\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # I guess this tells the model that\n    # paramteres are free to update\n    \n    model.train()\n    # for each batch of samples\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation and parameter update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Occasional progress report (like callback)\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:\u003e7f}  [{current:\u003e5d}/{size:\u003e5d}]\")\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"what-does-modeltrain-do\"\u003e\u003ca href=\"https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\"\u003eWhat Does model.train() do?\u003c/a\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-does-modeltrain-do\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIt sets the model so layers that behave differently during train and test set.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emodel.train()\u003c/code\u003e and \u003ccode\u003emodel.eval()\u003c/code\u003e basically only care about dropout and batch normalization layers atm.\u003c/p\u003e\n\u003cpre class=\"language-{python}\"\u003e\u003ccode class=\"language-{python}\"\u003edata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"lossbackward\"\u003e\u003ca href=\"https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\"\u003eloss.backward()\u003c/a\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lossbackward\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003eloss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2\"\u003e\u003c/p\u003e\n\u003cpre class=\"language-{python}\"\u003e\u003ccode class=\"language-{python}\"\u003ex.grad += dloss/dx\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo because of that we have to 0 the grad beforehand\u003c/p\u003e\n\u003ch1 id=\"grad-mode\"\u003eGrad Mode\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#grad-mode\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e(Enable grad)[https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html# enable_grad]\u003c/p\u003e\n\u003ch2 id=\"torchno_grad\"\u003eTorch.no_grad\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#torchno_grad\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eContext manager, disabling (thread-wise) grad calculations. Use when evaluating model.\u003c/p\u003e\n\u003ch2 id=\"aitem\"\u003ea.item()\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#aitem\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eif object is number/1 element tensor.\nuse a.tolist() otherwise.\u003c/p\u003e\n\u003ch1 id=\"differentiable\"\u003eDifferentiable\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#differentiable\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eSome ops in torch are differentiable, some are not.\u003c/p\u003e\n\u003ch1 id=\"tensors-and-differeentiataion\"\u003eTensors and differeentiataion\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensors-and-differeentiataion\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\"\u003e'Autograd' is some engine,running in the background? Idk\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"resources\"\u003eresources\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#resources\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\"\u003ePytorch Internals Blog Post\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1\"\u003eDeep Neural Networks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/PublicPersonalKnowledgeBase/notes/K93cKDmInFmPJGz1VVNTF\"\u003eDifferentiable and Probabalistic Programming\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1647507231129,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"db285659ccac8b133c384de1ef51de66","links":[],"anchors":{"welcome-to-stefans-notes":{"type":"header","text":"Welcome to Stefan's Notes!","value":"welcome-to-stefans-notes","line":7,"column":0,"depth":1}},"children":["mxzxxu8z4e6krz99ht96y9d","W1EOZ27Tqx6RbiA2aW3DI","DyoLE2kwm9rRfRZhBGxPW","idhfogizmtcmvaamtalp3o3","4w8wBCSRvYUnGIZeozW03","vB321AipYCs6ldVC0APs9","h6WVdl1UTWVXeWuMJSZ1f","bth6m0exy9q9loxib1mc4al","4abAmH56ausbldEJbZokx","JyOFJ5NTPSVWMfiDy951X","0OO7fjCpcaGZg5qDRZr8z","138666663","pohXgII67dAxnoufG7yAP","6hs48bnjnaoxahk07exj74u","42r6290iqzLPmg9BY7fIp","3hoLerNJHjNkDziIKlFF2","T6meT3UNw0nRorEbzoPSl","hIOTXIIBj3vmhG1xc91lA","tnFlQuOAGPkbU2fZI7Cb1","Jb3w3f4x8kixLhrjUW6S1","LV6q5jlD2xtCF6yYFEqFC","ENDcCZFjAW9h66eDoFg7I","b5IeREnsTbeggC7rmWV0p","2hmbhdzcdljtdwln762gcrv","z9la6u9t3xueldj2omf2gc6","9akeo93l6b026jmu4t6e7pw","sc24o4jglr9jg29qr5v0e44","0mt5ao8tbbz3z5mdwe0aer8","r7rvb6nal69nfb6ogqdadnm","xujx5iuxskj10o0ajpi109n","lyl1rzqz6zwcswpwh0kafrm","skpw697vlqdq3t3uqxpq5e3","a4tts3oc3oms7wuralrbzdc","13a1ufqh6o8yi9evw1u5kv5"],"parent":null,"data":{},"body":"# Welcome to Stefan's Notes!\n\n        \n\nLast updated: 2021.12.30\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/PublicPersonalKnowledgeBase","siteUrl":"https://petrovs12.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"awidUgzHuMsvTiKstZ28g"},"buildId":"d47oHCd-7QAOR5IvSuJke","assetPrefix":"/PublicPersonalKnowledgeBase","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>