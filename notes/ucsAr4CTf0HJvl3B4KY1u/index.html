<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/PublicPersonalKnowledgeBase/favicon.ico"/><title>Vision Models</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Vision Models"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://petrovs12.github.io/PublicPersonalKnowledgeBase/notes/ucsAr4CTf0HJvl3B4KY1u/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2/12/2022"/><meta property="article:modified_time" content="2/12/2022"/><link rel="canonical" href="https://petrovs12.github.io/PublicPersonalKnowledgeBase/notes/ucsAr4CTf0HJvl3B4KY1u/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/PublicPersonalKnowledgeBase/_next/static/css/7719b08cc3352912.css" as="style"/><link rel="stylesheet" href="/PublicPersonalKnowledgeBase/_next/static/css/7719b08cc3352912.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/PublicPersonalKnowledgeBase/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/webpack-b320c149a25b986a.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/main-8915ee191b7710e8.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/pages/_app-bf98b80149ea852b.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/d47oHCd-7QAOR5IvSuJke/_buildManifest.js" defer=""></script><script src="/PublicPersonalKnowledgeBase/_next/static/d47oHCd-7QAOR5IvSuJke/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="vision-models">Vision Models<a aria-hidden="true" class="anchor-heading icon-link" href="#vision-models"></a></h1>
<p><a href="/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1#^cnn">Deep Neural Networks</a></p>
<h1 id="image-classification">Image Classification<a aria-hidden="true" class="anchor-heading icon-link" href="#image-classification"></a></h1>
<p>Normally these models are trained on ImageNet and top-5 classification accuracy is reported.</p>
<p>Here are some of the current champions that are also available as <a href="/PublicPersonalKnowledgeBase/notes/fwJbQKdytxvuwjuehkoXf">ONNX</a> models.</p>
<h2 id="vgg-16">VGG-16<a aria-hidden="true" class="anchor-heading icon-link" href="#vgg-16"></a></h2>
<p><img src="/PublicPersonalKnowledgeBase/assets/images/2022-02-12-23-49-48.png">
<a href="https://keras.io/api/applications/vgg/">Keras Link</a></p>
<h2 id="resnet">ResNet<a aria-hidden="true" class="anchor-heading icon-link" href="#resnet"></a></h2>
<p>Uses Shortcut connections</p>
<h2 id="efficientnet-lite4">EfficientNet-Lite4<a aria-hidden="true" class="anchor-heading icon-link" href="#efficientnet-lite4"></a></h2>
<p><a href="https://arxiv.org/abs/1905.11946">Paper Link</a>
<a href="https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4">ONXX</a></p>
<h1 id="object-detection-and-image-segmentation">Object detection and Image Segmentation<a aria-hidden="true" class="anchor-heading icon-link" href="#object-detection-and-image-segmentation"></a></h1>
<p>Object detection and image segmentation algorithms draw rectangles on a picture and put labels on them.</p>
<h2 id="evaluation-metrics">Evaluation metrics<a aria-hidden="true" class="anchor-heading icon-link" href="#evaluation-metrics"></a></h2>
<p>A common evaluation metric are (from <a href="https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3">here</a>):</p>
<h3 id="intersection-over-union-iou">Intersection over Union (IoU):<a aria-hidden="true" class="anchor-heading icon-link" href="#intersection-over-union-iou"></a></h3>
<p> Evaluate the overlap of the two bounding boxes. Requires a true and predicted bounding box. So then we have something like:</p>
<p> Intersection Over Union on top 5 Labels.</p>
<h3 id="coco-map">COCO mAP<a aria-hidden="true" class="anchor-heading icon-link" href="#coco-map"></a></h3>
<p>For the COCO 2017 challenge, the mAP was calculated by averaging the AP over all 80 object categories AND all 10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05. The authors hypothesize that averaging over IoUs rewards detectors with better localization.</p>
<h2 id="modelsalgorithms">Models/Algorithms<a aria-hidden="true" class="anchor-heading icon-link" href="#modelsalgorithms"></a></h2>
<h3 id="yolo3">Yolo3<a aria-hidden="true" class="anchor-heading icon-link" href="#yolo3"></a></h3>
<p><a href="https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/yolov3">YoloV4</a></p>
<h1 id="body-face-and-gesture-analysis">Body, Face, And Gesture Analysis<a aria-hidden="true" class="anchor-heading icon-link" href="#body-face-and-gesture-analysis"></a></h1>
<h1 id="image-mnanipulation">Image MNanipulation<a aria-hidden="true" class="anchor-heading icon-link" href="#image-mnanipulation"></a></h1>
<ul>
<li>Unpaired Image To Image Translation</li>
<li>Super REsolution </li>
<li>Fast Neural Style Transfer</li>
</ul>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/V4exKlI4VAWINpSZOytYw">Transfer Learning</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/hiuvax4qcz63e0zseg5c8ai">Industry Applications</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#image-classification" title="Image Classification">Image Classification</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#vgg-16" title="VGG-16">VGG-16</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#resnet" title="ResNet">ResNet</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#efficientnet-lite4" title="EfficientNet-Lite4">EfficientNet-Lite4</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#object-detection-and-image-segmentation" title="Object detection and Image Segmentation">Object detection and Image Segmentation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#evaluation-metrics" title="Evaluation metrics">Evaluation metrics</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#intersection-over-union-iou" title="Intersection over Union (IoU):">Intersection over Union (IoU):</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#coco-map" title="COCO mAP">COCO mAP</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#modelsalgorithms" title="Models/Algorithms">Models/Algorithms</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#yolo3" title="Yolo3">Yolo3</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#body-face-and-gesture-analysis" title="Body, Face, And Gesture Analysis">Body, Face, And Gesture Analysis</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#image-mnanipulation" title="Image MNanipulation">Image MNanipulation</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"ucsAr4CTf0HJvl3B4KY1u","title":"Vision Models","desc":"","updated":1644708282688,"created":1644706160543,"custom":{},"fname":"science.stats.Deep Neural Networks.Vision Models","type":"note","vault":{"fsPath":"vault"},"contentHash":"3f09c90b05ff927085aa4f0a4f4697f5","links":[{"type":"wiki","from":{"fname":"science.stats.Deep Neural Networks.Vision Models","id":"ucsAr4CTf0HJvl3B4KY1u","vaultName":"vault"},"value":"science.stats.Deep Neural Networks","position":{"start":{"line":3,"column":1,"offset":2},"end":{"line":3,"column":44,"offset":45},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"science.stats.Deep Neural Networks","anchorHeader":"^cnn"}},{"type":"wiki","from":{"fname":"science.stats.Deep Neural Networks.Vision Models","id":"ucsAr4CTf0HJvl3B4KY1u","vaultName":"vault"},"value":"science.engineering.technologies.MLOps.ONNX","position":{"start":{"line":10,"column":67,"offset":232},"end":{"line":10,"column":114,"offset":279},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"science.engineering.technologies.MLOps.ONNX"}},{"from":{"fname":"science.stats.Deep Neural Networks.Transfer Learning","id":"V4exKlI4VAWINpSZOytYw","vaultName":"vault"},"type":"backlink","position":{"start":{"line":8,"column":1,"offset":140},"end":{"line":8,"column":53,"offset":192},"indent":[]},"value":"science.stats.Deep Neural Networks.Vision Models"},{"from":{"fname":"science.stats.Machine Learning.Industry Applications","id":"hiuvax4qcz63e0zseg5c8ai","vaultName":"vault"},"type":"backlink","position":{"start":{"line":18,"column":1,"offset":816},"end":{"line":18,"column":53,"offset":868},"indent":[]},"value":"science.stats.Deep Neural Networks.Vision Models"}],"anchors":{"image-classification":{"type":"header","text":"Image Classification","value":"image-classification","line":12,"column":0,"depth":1},"vgg-16":{"type":"header","text":"VGG-16","value":"vgg-16","line":18,"column":0,"depth":2},"resnet":{"type":"header","text":"ResNet","value":"resnet","line":23,"column":0,"depth":2},"efficientnet-lite4":{"type":"header","text":"EfficientNet-Lite4","value":"efficientnet-lite4","line":27,"column":0,"depth":2},"object-detection-and-image-segmentation":{"type":"header","text":"Object detection and Image Segmentation","value":"object-detection-and-image-segmentation","line":33,"column":0,"depth":1},"evaluation-metrics":{"type":"header","text":"Evaluation metrics","value":"evaluation-metrics","line":38,"column":0,"depth":2},"intersection-over-union-iou":{"type":"header","text":"Intersection over Union (IoU):","value":"intersection-over-union-iou","line":41,"column":0,"depth":3},"coco-map":{"type":"header","text":"COCO mAP","value":"coco-map","line":47,"column":0,"depth":3},"modelsalgorithms":{"type":"header","text":"Models/Algorithms","value":"modelsalgorithms","line":51,"column":0,"depth":2},"yolo3":{"type":"header","text":"Yolo3","value":"yolo3","line":53,"column":0,"depth":3},"body-face-and-gesture-analysis":{"type":"header","text":"Body, Face, And Gesture Analysis","value":"body-face-and-gesture-analysis","line":57,"column":0,"depth":1},"image-mnanipulation":{"type":"header","text":"Image MNanipulation","value":"image-mnanipulation","line":60,"column":0,"depth":1}},"children":[],"parent":"1oUqI7ql4EuPu9Ml94Xe1","data":{}},"body":"\u003ch1 id=\"vision-models\"\u003eVision Models\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#vision-models\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1#^cnn\"\u003eDeep Neural Networks\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"image-classification\"\u003eImage Classification\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#image-classification\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eNormally these models are trained on ImageNet and top-5 classification accuracy is reported.\u003c/p\u003e\n\u003cp\u003eHere are some of the current champions that are also available as \u003ca href=\"/PublicPersonalKnowledgeBase/notes/fwJbQKdytxvuwjuehkoXf\"\u003eONNX\u003c/a\u003e models.\u003c/p\u003e\n\u003ch2 id=\"vgg-16\"\u003eVGG-16\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#vgg-16\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/PublicPersonalKnowledgeBase/assets/images/2022-02-12-23-49-48.png\"\u003e\n\u003ca href=\"https://keras.io/api/applications/vgg/\"\u003eKeras Link\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"resnet\"\u003eResNet\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#resnet\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eUses Shortcut connections\u003c/p\u003e\n\u003ch2 id=\"efficientnet-lite4\"\u003eEfficientNet-Lite4\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#efficientnet-lite4\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1905.11946\"\u003ePaper Link\u003c/a\u003e\n\u003ca href=\"https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4\"\u003eONXX\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"object-detection-and-image-segmentation\"\u003eObject detection and Image Segmentation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#object-detection-and-image-segmentation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eObject detection and image segmentation algorithms draw rectangles on a picture and put labels on them.\u003c/p\u003e\n\u003ch2 id=\"evaluation-metrics\"\u003eEvaluation metrics\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#evaluation-metrics\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eA common evaluation metric are (from \u003ca href=\"https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3\"\u003ehere\u003c/a\u003e):\u003c/p\u003e\n\u003ch3 id=\"intersection-over-union-iou\"\u003eIntersection over Union (IoU):\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#intersection-over-union-iou\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e Evaluate the overlap of the two bounding boxes. Requires a true and predicted bounding box. So then we have something like:\u003c/p\u003e\n\u003cp\u003e Intersection Over Union on top 5 Labels.\u003c/p\u003e\n\u003ch3 id=\"coco-map\"\u003eCOCO mAP\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#coco-map\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eFor the COCO 2017 challenge, the mAP was calculated by averaging the AP over all 80 object categories AND all 10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05. The authors hypothesize that averaging over IoUs rewards detectors with better localization.\u003c/p\u003e\n\u003ch2 id=\"modelsalgorithms\"\u003eModels/Algorithms\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#modelsalgorithms\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"yolo3\"\u003eYolo3\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#yolo3\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/yolov3\"\u003eYoloV4\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"body-face-and-gesture-analysis\"\u003eBody, Face, And Gesture Analysis\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#body-face-and-gesture-analysis\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch1 id=\"image-mnanipulation\"\u003eImage MNanipulation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#image-mnanipulation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eUnpaired Image To Image Translation\u003c/li\u003e\n\u003cli\u003eSuper REsolution \u003c/li\u003e\n\u003cli\u003eFast Neural Style Transfer\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/PublicPersonalKnowledgeBase/notes/V4exKlI4VAWINpSZOytYw\"\u003eTransfer Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/PublicPersonalKnowledgeBase/notes/hiuvax4qcz63e0zseg5c8ai\"\u003eIndustry Applications\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1647507231129,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"db285659ccac8b133c384de1ef51de66","links":[],"anchors":{"welcome-to-stefans-notes":{"type":"header","text":"Welcome to Stefan's Notes!","value":"welcome-to-stefans-notes","line":7,"column":0,"depth":1}},"children":["mxzxxu8z4e6krz99ht96y9d","W1EOZ27Tqx6RbiA2aW3DI","DyoLE2kwm9rRfRZhBGxPW","idhfogizmtcmvaamtalp3o3","4w8wBCSRvYUnGIZeozW03","vB321AipYCs6ldVC0APs9","h6WVdl1UTWVXeWuMJSZ1f","bth6m0exy9q9loxib1mc4al","4abAmH56ausbldEJbZokx","JyOFJ5NTPSVWMfiDy951X","0OO7fjCpcaGZg5qDRZr8z","138666663","pohXgII67dAxnoufG7yAP","6hs48bnjnaoxahk07exj74u","42r6290iqzLPmg9BY7fIp","3hoLerNJHjNkDziIKlFF2","T6meT3UNw0nRorEbzoPSl","hIOTXIIBj3vmhG1xc91lA","tnFlQuOAGPkbU2fZI7Cb1","Jb3w3f4x8kixLhrjUW6S1","LV6q5jlD2xtCF6yYFEqFC","ENDcCZFjAW9h66eDoFg7I","b5IeREnsTbeggC7rmWV0p","2hmbhdzcdljtdwln762gcrv","z9la6u9t3xueldj2omf2gc6","9akeo93l6b026jmu4t6e7pw","sc24o4jglr9jg29qr5v0e44","0mt5ao8tbbz3z5mdwe0aer8","r7rvb6nal69nfb6ogqdadnm","xujx5iuxskj10o0ajpi109n","lyl1rzqz6zwcswpwh0kafrm","skpw697vlqdq3t3uqxpq5e3","a4tts3oc3oms7wuralrbzdc","13a1ufqh6o8yi9evw1u5kv5"],"parent":null,"data":{},"body":"# Welcome to Stefan's Notes!\n\n        \n\nLast updated: 2021.12.30\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/PublicPersonalKnowledgeBase","siteUrl":"https://petrovs12.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"ucsAr4CTf0HJvl3B4KY1u"},"buildId":"d47oHCd-7QAOR5IvSuJke","assetPrefix":"/PublicPersonalKnowledgeBase","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>