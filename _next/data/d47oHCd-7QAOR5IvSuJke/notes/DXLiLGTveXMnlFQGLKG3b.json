{"pageProps":{"note":{"id":"DXLiLGTveXMnlFQGLKG3b","title":"Regularizations","desc":"","updated":1646298983624,"created":1641864475560,"custom":{},"fname":"science.math.Optimization.Regularizations","type":"note","vault":{"fsPath":"vault"},"contentHash":"bb5ce3098dabd8e5d5977361840f664e","links":[{"type":"wiki","from":{"fname":"science.math.Optimization.Regularizations","id":"DXLiLGTveXMnlFQGLKG3b","vaultName":"vault"},"value":"science.stats.Regression.Linear Regression","position":{"start":{"line":2,"column":1,"offset":46},"end":{"line":2,"column":47,"offset":92},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"science.stats.Regression.Linear Regression"}},{"type":"wiki","from":{"fname":"science.math.Optimization.Regularizations","id":"DXLiLGTveXMnlFQGLKG3b","vaultName":"vault"},"value":"science.stats.Regression.Linear Regression","position":{"start":{"line":5,"column":1,"offset":107},"end":{"line":5,"column":47,"offset":153},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"science.stats.Regression.Linear Regression"}},{"type":"wiki","from":{"fname":"science.math.Optimization.Regularizations","id":"DXLiLGTveXMnlFQGLKG3b","vaultName":"vault"},"value":"science.math.Norms and Metrics","position":{"start":{"line":17,"column":1,"offset":1040},"end":{"line":17,"column":35,"offset":1074},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"science.math.Norms and Metrics"}},{"type":"ref","from":{"fname":"science.math.Optimization.Regularizations","id":"DXLiLGTveXMnlFQGLKG3b","vaultName":"vault"},"value":"science.math.Optimization.Regularizations.Dropout","position":{"start":{"line":20,"column":1,"offset":1086},"end":{"line":20,"column":55,"offset":1140},"indent":[]},"xvault":false,"to":{"fname":"science.math.Optimization.Regularizations.Dropout"}}],"anchors":{"l_1or-l_2-regularizations-in-regression":{"type":"header","text":"$L_1$or $L_2$ regularizations in regression","value":"l_1or-l_2-regularizations-in-regression","line":7,"column":0,"depth":1},"map-vs-mle":{"type":"header","text":"MAP vs MLE","value":"map-vs-mle","line":10,"column":0,"depth":1},"l1-l2-ridge-tikhonov-lasso":{"type":"header","text":"L1, L2, Ridge, Tikhonov, LASSO","value":"l1-l2-ridge-tikhonov-lasso","line":19,"column":0,"depth":1},"dropout":{"type":"header","text":"Dropout","value":"dropout","line":25,"column":0,"depth":1}},"children":["ytp3nmwv2nmc0tum4472bba"],"parent":"4FIxH4rGwMUvVyCogNmHe","data":{}},"body":"<h1 id=\"regularizations\">Regularizations<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#regularizations\"></a></h1>\n<h1 id=\"l_1or-l_2-regularizations-in-regression\"><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">L_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>or <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">L_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> regularizations in regression<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#l_1or-l_2-regularizations-in-regression\"></a></h1>\n<p><a href=\"/PublicPersonalKnowledgeBase/notes/YSgeu1I7ZeKOAKUbvuL8M\">Linear Regression</a></p>\n<h1 id=\"map-vs-mle\">MAP vs MLE<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#map-vs-mle\"></a></h1>\n<p><a href=\"/PublicPersonalKnowledgeBase/notes/YSgeu1I7ZeKOAKUbvuL8M\">Linear Regression</a></p>\n<p>Given a dataset X, a common task is to try to estimate the most likely values for the model parameters. To do this, you must find the values that maximize the likelihood function, given X. In this example, if you have observed a single instance x=2.5, the maximum likelihood estimate (MLE) of θ is <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>θ</mi><mo>^</mo></mover><mo>=</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">\\hat{\\theta}=1.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9579em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9579em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span></span><span style=\"top:-3.2634em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.1667em;\"><span class=\"mord\">^</span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">1.5</span></span></span></span></span>. If a prior probability distribution g over θ exists, it is possible to take it into account by maximizing <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo><mi>g</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L(θ|x)g(θ)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span> rather than just maximizing <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>L</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">L(θ|x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">L</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span>. This is called <strong>maximum a-posteriori (MAP) estimation</strong>. Since MAP constrains the parameter values, you can think of it as a regularized version of MLE.</p>\n<p>For example <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">L_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> regularization is equivalent to putting a Laplace prior on the parameters, whereas\n<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">L_2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">L</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span> regularization is equivalent to putting a Gaussian prior on them.</p>\n<h1 id=\"l1-l2-ridge-tikhonov-lasso\">L1, L2, Ridge, Tikhonov, LASSO<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#l1-l2-ridge-tikhonov-lasso\"></a></h1>\n<p>Ridge regression has a closed form solution.</p>\n<p><a href=\"/PublicPersonalKnowledgeBase/notes/1JYjLMyPvKFb3dG4hc3oM\">Norms and Metrics</a></p>\n<h1 id=\"dropout\">Dropout<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dropout\"></a></h1>\n<p></p><p></p><div class=\"portal-container\">\n<div class=\"portal-head\">\n<div class=\"portal-backlink\">\n<div class=\"portal-title\">From <span class=\"portal-text-title\">Dropout</span></div>\n<a href=\"/PublicPersonalKnowledgeBase/notes/ytp3nmwv2nmc0tum4472bba\" class=\"portal-arrow\">Go to text <span class=\"right-arrow\">→</span></a>\n</div>\n</div>\n<div id=\"portal-parent-anchor\" class=\"portal-parent\" markdown=\"1\">\n<div class=\"portal-parent-fader-top\"></div>\n<div class=\"portal-parent-fader-bottom\"></div><p>Dropout:</p>\n<ol>\n<li>At training time, in each training epoch, drop out nodes/connections randomly by setting their weights to zero.\nEach node weight is set by 0 with probability <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span></span></span></span></span>.</li>\n<li>At testing/inference time, multiply each learned weight by <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span></span></span></span></span>.</li>\n</ol>\n<p>Justification:\nWe 'stochastically train' simultaneously <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mrow><mi mathvariant=\"normal\">∣</mi><mi>N</mi><mi>N</mi><mi mathvariant=\"normal\">∣</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2^{|NN|}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.888em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.888em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">NN</span><span class=\"mord mtight\">∣</span></span></span></span></span></span></span></span></span></span></span></span></span> models,where <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi>N</mi><mi>N</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|NN|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">NN</span><span class=\"mord\">∣</span></span></span></span></span> is the network size.\nThus at the end of training we may pretend we have <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mn>2</mn><mrow><mi mathvariant=\"normal\">∣</mi><mi>N</mi><mi>N</mi><mi mathvariant=\"normal\">∣</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2^{|NN|}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.888em;\"></span><span class=\"mord\"><span class=\"mord\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.888em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">NN</span><span class=\"mord mtight\">∣</span></span></span></span></span></span></span></span></span></span></span></span></span> candidate models. That is, we can drop any number of nodes and still have a model.\nNow, ideally, on inference time, we may :\nFor each subset of nodes, disable them, and predict on the basis of the rest, and then take the average.\nBut this is obviously infeasible.  So we approximate their 'model average' by <strong>multiplying the weights by <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span></span></span></span></span></strong>.</p>\n</div></div><p></p><p></p>\n<hr>\n<strong>Children</strong>\n<ol>\n<li><a href=\"/PublicPersonalKnowledgeBase/notes/ytp3nmwv2nmc0tum4472bba\">Dropout</a></li>\n</ol>","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1647507231129,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"db285659ccac8b133c384de1ef51de66","links":[],"anchors":{"welcome-to-stefans-notes":{"type":"header","text":"Welcome to Stefan's Notes!","value":"welcome-to-stefans-notes","line":7,"column":0,"depth":1}},"children":["mxzxxu8z4e6krz99ht96y9d","W1EOZ27Tqx6RbiA2aW3DI","DyoLE2kwm9rRfRZhBGxPW","idhfogizmtcmvaamtalp3o3","4w8wBCSRvYUnGIZeozW03","vB321AipYCs6ldVC0APs9","h6WVdl1UTWVXeWuMJSZ1f","bth6m0exy9q9loxib1mc4al","4abAmH56ausbldEJbZokx","JyOFJ5NTPSVWMfiDy951X","0OO7fjCpcaGZg5qDRZr8z","138666663","pohXgII67dAxnoufG7yAP","6hs48bnjnaoxahk07exj74u","42r6290iqzLPmg9BY7fIp","3hoLerNJHjNkDziIKlFF2","T6meT3UNw0nRorEbzoPSl","hIOTXIIBj3vmhG1xc91lA","tnFlQuOAGPkbU2fZI7Cb1","Jb3w3f4x8kixLhrjUW6S1","LV6q5jlD2xtCF6yYFEqFC","ENDcCZFjAW9h66eDoFg7I","b5IeREnsTbeggC7rmWV0p","2hmbhdzcdljtdwln762gcrv","z9la6u9t3xueldj2omf2gc6","9akeo93l6b026jmu4t6e7pw","sc24o4jglr9jg29qr5v0e44","0mt5ao8tbbz3z5mdwe0aer8","r7rvb6nal69nfb6ogqdadnm","xujx5iuxskj10o0ajpi109n","lyl1rzqz6zwcswpwh0kafrm","skpw697vlqdq3t3uqxpq5e3","a4tts3oc3oms7wuralrbzdc","13a1ufqh6o8yi9evw1u5kv5"],"parent":null,"data":{},"body":"# Welcome to Stefan's Notes!\n\n        \n\nLast updated: 2021.12.30\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/PublicPersonalKnowledgeBase","siteUrl":"https://petrovs12.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}