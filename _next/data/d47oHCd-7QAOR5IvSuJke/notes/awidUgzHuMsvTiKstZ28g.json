{"pageProps":{"note":{"id":"awidUgzHuMsvTiKstZ28g","title":"pytorch","desc":"","updated":1642039817519,"created":1641834743300,"custom":{},"fname":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch","type":"note","vault":{"fsPath":"vault"},"contentHash":"b1dc7c6de18892ea9cb0fbd557a44889","links":[{"type":"wiki","from":{"fname":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch","id":"awidUgzHuMsvTiKstZ28g","vaultName":"vault"},"value":"tags.NB","alias":"#NB","position":{"start":{"line":45,"column":8,"offset":1124},"end":{"line":45,"column":11,"offset":1127},"indent":[]},"xvault":false,"to":{"fname":"tags.NB"}},{"from":{"fname":"science.stats.Deep Neural Networks","id":"1oUqI7ql4EuPu9Ml94Xe1","vaultName":"vault"},"type":"backlink","position":{"start":{"line":3,"column":1,"offset":82},"end":{"line":3,"column":78,"offset":159},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"},{"from":{"fname":"science.stats.Deep Neural Networks","id":"1oUqI7ql4EuPu9Ml94Xe1","vaultName":"vault"},"type":"backlink","position":{"start":{"line":115,"column":2,"offset":2919},"end":{"line":115,"column":79,"offset":2996},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"},{"from":{"fname":"science.CS.theory.Code Transformations.Differentiable and Probabalistic Programming","id":"K93cKDmInFmPJGz1VVNTF","vaultName":"vault"},"type":"backlink","position":{"start":{"line":4,"column":1,"offset":83},"end":{"line":4,"column":78,"offset":160},"indent":[]},"value":"science.CS.theory.Code Transformations.Differentiable Programming.pytorch"}],"anchors":{"multiple-submodules":{"type":"header","text":"Multiple submodules","value":"multiple-submodules","line":10,"column":0,"depth":1},"dataloader":{"type":"header","text":"DataLoader","value":"dataloader","line":12,"column":0,"depth":2},"neural-nets-forward-and-backwards":{"type":"header","text":"Neural Nets, Forward and Backwards","value":"neural-nets-forward-and-backwards","line":17,"column":0,"depth":1},"parameter-fittingoptimization-and-backward-pass":{"type":"header","text":"Parameter fitting/optimization and backward pass","value":"parameter-fittingoptimization-and-backward-pass","line":46,"column":0,"depth":2},"training-loop":{"type":"header","text":"Training loop","value":"training-loop","line":62,"column":0,"depth":1},"what-does-modeltrain-do":{"type":"header","text":"What Does model.train() do?","value":"what-does-modeltrain-do","line":116,"column":0,"depth":2},"lossbackward":{"type":"header","text":"loss.backward()","value":"lossbackward","line":128,"column":0,"depth":1},"grad-mode":{"type":"header","text":"Grad Mode","value":"grad-mode","line":144,"column":0,"depth":1},"torchno_grad":{"type":"header","text":"Torch.no_grad","value":"torchno_grad","line":148,"column":0,"depth":2},"aitem":{"type":"header","text":"a.item()","value":"aitem","line":152,"column":0,"depth":2},"differentiable":{"type":"header","text":"Differentiable","value":"differentiable","line":156,"column":0,"depth":1},"tensors-and-differeentiataion":{"type":"header","text":"Tensors and differeentiataion","value":"tensors-and-differeentiataion","line":159,"column":0,"depth":1},"resources":{"type":"header","text":"resources","value":"resources","line":164,"column":0,"depth":1}},"children":[],"parent":"qnsew4i2mrvs1xsb4f2zktp","data":{}},"body":"<h1 id=\"pytorch\">pytorch<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pytorch\"></a></h1>\n<p><a href=\"https://pytorch.org/\">Website</a></p>\n<h1 id=\"multiple-submodules\">Multiple submodules<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#multiple-submodules\"></a></h1>\n<h2 id=\"dataloader\">DataLoader<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dataloader\"></a></h2>\n<p>It's a iterator helper that would deal w/</p>\n<h1 id=\"neural-nets-forward-and-backwards\">Neural Nets, Forward and Backwards<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#neural-nets-forward-and-backwards\"></a></h1>\n<p> To create a neural net, make a class that inherits from <code>nn.Module</code>.\nIt needs to implement <code>__init__</code> and <code>forward</code> methods.</p>\n<pre class=\"language-{python}\"><code class=\"language-{python}\">class DenseNet(nn.Module):\n   def __init__():\n       self.input = nn.Flatten()\n       self.linear_relu_stack=nn.Sequential(\n           nn.Linear(28*25, 256),\n           nn.ReLU(),\n           nn.Linear(256, 128),\n           nn.ReLU(),\n           nn.Linear(128, 10)\n       )\n   def forward(self,x):\n       x = self.input(x)\n       logits = self.linear_relu_stack(x)\n       return logits\n\nmodel = DenseNet().to(device)# send the model to execute on the gpu/cpu\n\n\n</code></pre>\n<p> So that's how we encode a a given computational tree.\nProbably there are various simplifications/specializations of the above. Is</p>\n<h2 id=\"parameter-fittingoptimization-and-backward-pass\">Parameter fitting/optimization and backward pass<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#parameter-fittingoptimization-and-backward-pass\"></a></h2>\n<p>After we have defined the 'evaluation' part, next we want to see how to fit parameters to the data.</p>\n<p>Define 'optimizer' object and the loss funciton object\n<strong>NB</strong> <a title=\"Private\" href=\"https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html\" target=\"_blank\" class=\"private\">#NB (Private)</a> optimizer needs to be supplied the model.parameters() as an argument in order to know what it's optimizing.</p>\n<p>If you think about a function instead, the optimizer\nwould need to know the function + it's parameters, so it makes sense.</p>\n<pre class=\"language-{python}\"><code class=\"language-{python}\">loss_fn = nn.CrossEntropyLoss()\n!!!\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</code></pre>\n<h1 id=\"training-loop\">Training loop<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#training-loop\"></a></h1>\n<p>We have to define the training loop by ourselves.</p>\n<p>The backpropagation is done via the following opaque at first look pattern:</p>\n<pre class=\"language-{python}\"><code class=\"language-{python}\">optimizer.zero_grad() # nullify previous gradient data\nloss.backward() #compute gradient (!!!)\noptimizer.step() #make step w/ the optimizer\n</code></pre>\n<p>Overall  Ifind the above pattern confusing, as there is quite a bit of state manipulation there.\nIn particular, the signatures of functions there don't tell you about what's happening.</p>\n<p> Nevertheless, it's short and only 'strange' part is</p>\n<p><code>loss.backward()</code>. Why is this actinf on the model parameters? no idea.</p>\n<p>from <a href=\"http://seba1511.net/tutorials/beginner/blitz/neural_networks_tutorial.html#:~:text=Loss%20Function,-A%20loss%20function&#x26;text=MSELoss%20which%20computes%20the%20mean,the%20input%20and%20the%20target.&#x26;text=So%2C%20when%20we%20call%20loss,Variable%20accumulated%20with%20the%20gradient.\">here</a></p>\n<p><strong>So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Variables in the graph will have their .grad Variable accumulated with the gradient.</strong></p>\n<p> Ok, so that makes sense. Every Variable has a .grad Variable, which is acted upon by <code>loss.backward()</code>.</p>\n<pre><code>\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # I guess this tells the model that\n    # paramteres are free to update\n    \n    model.train()\n    # for each batch of samples\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation and parameter update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Occasional progress report (like callback)\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n</code></pre>\n<h2 id=\"what-does-modeltrain-do\"><a href=\"https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\">What Does model.train() do?</a><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#what-does-modeltrain-do\"></a></h2>\n<p>It sets the model so layers that behave differently during train and test set.</p>\n<p><code>model.train()</code> and <code>model.eval()</code> basically only care about dropout and batch normalization layers atm.</p>\n<pre class=\"language-{python}\"><code class=\"language-{python}\">data = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n</code></pre>\n<h1 id=\"lossbackward\"><a href=\"https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944\">loss.backward()</a><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lossbackward\"></a></h1>\n<pre><code>loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n</code></pre>\n<p><img src=\"https://discuss.pytorch.org/t/what-does-the-backward-function-do/9944/2\"></p>\n<pre class=\"language-{python}\"><code class=\"language-{python}\">x.grad += dloss/dx\n\n</code></pre>\n<p>So because of that we have to 0 the grad beforehand</p>\n<h1 id=\"grad-mode\">Grad Mode<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#grad-mode\"></a></h1>\n<p>(Enable grad)[https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html# enable_grad]</p>\n<h2 id=\"torchno_grad\">Torch.no_grad<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#torchno_grad\"></a></h2>\n<p>Context manager, disabling (thread-wise) grad calculations. Use when evaluating model.</p>\n<h2 id=\"aitem\">a.item()<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#aitem\"></a></h2>\n<p>if object is number/1 element tensor.\nuse a.tolist() otherwise.</p>\n<h1 id=\"differentiable\">Differentiable<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#differentiable\"></a></h1>\n<p>Some ops in torch are differentiable, some are not.</p>\n<h1 id=\"tensors-and-differeentiataion\">Tensors and differeentiataion<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tensors-and-differeentiataion\"></a></h1>\n<p><a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">'Autograd' is some engine,running in the background? Idk</a></p>\n<h1 id=\"resources\">resources<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#resources\"></a></h1>\n<p><a href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\">Pytorch Internals Blog Post</a></p>\n<hr>\n<strong>Backlinks</strong>\n<ul>\n<li><a href=\"/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1\">Deep Neural Networks</a></li>\n<li><a href=\"/PublicPersonalKnowledgeBase/notes/K93cKDmInFmPJGz1VVNTF\">Differentiable and Probabalistic Programming</a></li>\n</ul>","noteIndex":{"id":"wn8PE1RhG0znK1alrGFYv","title":"Root","desc":"","updated":1647507231129,"created":1631901573363,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"db285659ccac8b133c384de1ef51de66","links":[],"anchors":{"welcome-to-stefans-notes":{"type":"header","text":"Welcome to Stefan's Notes!","value":"welcome-to-stefans-notes","line":7,"column":0,"depth":1}},"children":["mxzxxu8z4e6krz99ht96y9d","W1EOZ27Tqx6RbiA2aW3DI","DyoLE2kwm9rRfRZhBGxPW","idhfogizmtcmvaamtalp3o3","4w8wBCSRvYUnGIZeozW03","vB321AipYCs6ldVC0APs9","h6WVdl1UTWVXeWuMJSZ1f","bth6m0exy9q9loxib1mc4al","4abAmH56ausbldEJbZokx","JyOFJ5NTPSVWMfiDy951X","0OO7fjCpcaGZg5qDRZr8z","138666663","pohXgII67dAxnoufG7yAP","6hs48bnjnaoxahk07exj74u","42r6290iqzLPmg9BY7fIp","3hoLerNJHjNkDziIKlFF2","T6meT3UNw0nRorEbzoPSl","hIOTXIIBj3vmhG1xc91lA","tnFlQuOAGPkbU2fZI7Cb1","Jb3w3f4x8kixLhrjUW6S1","LV6q5jlD2xtCF6yYFEqFC","ENDcCZFjAW9h66eDoFg7I","b5IeREnsTbeggC7rmWV0p","2hmbhdzcdljtdwln762gcrv","z9la6u9t3xueldj2omf2gc6","9akeo93l6b026jmu4t6e7pw","sc24o4jglr9jg29qr5v0e44","0mt5ao8tbbz3z5mdwe0aer8","r7rvb6nal69nfb6ogqdadnm","xujx5iuxskj10o0ajpi109n","lyl1rzqz6zwcswpwh0kafrm","skpw697vlqdq3t3uqxpq5e3","a4tts3oc3oms7wuralrbzdc","13a1ufqh6o8yi9evw1u5kv5"],"parent":null,"data":{},"body":"# Welcome to Stefan's Notes!\n\n        \n\nLast updated: 2021.12.30\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"dendronVersion":"0.95.0","enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false,"templateHierarchy":"template"},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"Personal knowledge space"},"github":{"enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","assetsPrefix":"/PublicPersonalKnowledgeBase","siteUrl":"https://petrovs12.github.io","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"theme":"light","templateVersion":"0.97.0","siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}