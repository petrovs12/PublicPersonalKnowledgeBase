<h1 id="stochastic-gradient-descent">Stochastic Gradient Descent<a aria-hidden="true" class="anchor-heading icon-link" href="#stochastic-gradient-descent"></a></h1>
<p><a href="https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a">This article claims SGD considers only 1 example at a time.</a></p>
<p><a href="https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent">This corraborates</a>.</p>
<p>So SGD takes only 1 example, batch takes all, minibatch takes a subset of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span> examples.
<a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">#minibatch (Private)</a> <a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">#batch (Private)</a></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/QE1BddXlccbyYN1avaB4M">Recommender Systems</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/GiPKRG8QmHoJSRp0Y4KrA">Gradient Descent</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/gz71FnqSIo2TfwivaD4Df">Optimizers In in Neural Networks</a></li>
</ul>