<h1 id="negative-sampling">Negative Sampling<a aria-hidden="true" class="anchor-heading icon-link" href="#negative-sampling"></a></h1>
<p>In a context where finally we have a 'classifier' that outputs a one-hot vector,
we have essencially a large-cardinality multinomial distribution.</p>
<p>the likelyhood has terms then for all possible categories. They can be many (e.g. <a href="/PublicPersonalKnowledgeBase/notes/sx92oju973uydTM0c39Cv#^word2vec">Language Models</a>).</p>
<p>Then, to speed up things, we pretend for this training example only that the multinomial distribution
is much smaller (e.g. pick 5 random 'negative' examples).
Then compute the loss and corresponding gradients (for this iteration) based on that.</p>
<p><a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/#:~:text=Negative%20sampling%20addresses%20this%20by%20having%20each%20training,output%E2%80%9D%20of%20the%20network%20is%20a%20one-hot%20vector.">Source: Mccormickml tutorial</a></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/QE1BddXlccbyYN1avaB4M">Recommender Systems</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/QTHWXTVUZOxYZPVJ0eh7Y">Implicit Data Handling</a></li>
</ul>