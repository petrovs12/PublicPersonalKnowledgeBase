<h1 id="vision-models">Vision Models<a aria-hidden="true" class="anchor-heading icon-link" href="#vision-models"></a></h1>
<p><a href="/PublicPersonalKnowledgeBase/notes/1oUqI7ql4EuPu9Ml94Xe1#^cnn">Deep Neural Networks</a></p>
<h1 id="image-classification">Image Classification<a aria-hidden="true" class="anchor-heading icon-link" href="#image-classification"></a></h1>
<p>Normally these models are trained on ImageNet and top-5 classification accuracy is reported.</p>
<p>Here are some of the current champions that are also available as <a href="/PublicPersonalKnowledgeBase/notes/fwJbQKdytxvuwjuehkoXf">ONNX</a> models.</p>
<h2 id="vgg-16">VGG-16<a aria-hidden="true" class="anchor-heading icon-link" href="#vgg-16"></a></h2>
<p><img src="/PublicPersonalKnowledgeBase/assets/images/2022-02-12-23-49-48.png">
<a href="https://keras.io/api/applications/vgg/">Keras Link</a></p>
<h2 id="resnet">ResNet<a aria-hidden="true" class="anchor-heading icon-link" href="#resnet"></a></h2>
<p>Uses Shortcut connections</p>
<h2 id="efficientnet-lite4">EfficientNet-Lite4<a aria-hidden="true" class="anchor-heading icon-link" href="#efficientnet-lite4"></a></h2>
<p><a href="https://arxiv.org/abs/1905.11946">Paper Link</a>
<a href="https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4">ONXX</a></p>
<h1 id="object-detection-and-image-segmentation">Object detection and Image Segmentation<a aria-hidden="true" class="anchor-heading icon-link" href="#object-detection-and-image-segmentation"></a></h1>
<p>Object detection and image segmentation algorithms draw rectangles on a picture and put labels on them.</p>
<h2 id="evaluation-metrics">Evaluation metrics<a aria-hidden="true" class="anchor-heading icon-link" href="#evaluation-metrics"></a></h2>
<p>A common evaluation metric are (from <a href="https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3">here</a>):</p>
<h3 id="intersection-over-union-iou">Intersection over Union (IoU):<a aria-hidden="true" class="anchor-heading icon-link" href="#intersection-over-union-iou"></a></h3>
<p> Evaluate the overlap of the two bounding boxes. Requires a true and predicted bounding box. So then we have something like:</p>
<p> Intersection Over Union on top 5 Labels.</p>
<h3 id="coco-map">COCO mAP<a aria-hidden="true" class="anchor-heading icon-link" href="#coco-map"></a></h3>
<p>For the COCO 2017 challenge, the mAP was calculated by averaging the AP over all 80 object categories AND all 10 IoU thresholds from 0.5 to 0.95 with a step size of 0.05. The authors hypothesize that averaging over IoUs rewards detectors with better localization.</p>
<h2 id="modelsalgorithms">Models/Algorithms<a aria-hidden="true" class="anchor-heading icon-link" href="#modelsalgorithms"></a></h2>
<h3 id="yolo3">Yolo3<a aria-hidden="true" class="anchor-heading icon-link" href="#yolo3"></a></h3>
<p><a href="https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/yolov3">YoloV4</a></p>
<h1 id="body-face-and-gesture-analysis">Body, Face, And Gesture Analysis<a aria-hidden="true" class="anchor-heading icon-link" href="#body-face-and-gesture-analysis"></a></h1>
<h1 id="image-mnanipulation">Image MNanipulation<a aria-hidden="true" class="anchor-heading icon-link" href="#image-mnanipulation"></a></h1>
<ul>
<li>Unpaired Image To Image Translation</li>
<li>Super REsolution </li>
<li>Fast Neural Style Transfer</li>
</ul>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/V4exKlI4VAWINpSZOytYw">Transfer Learning</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/hiuvax4qcz63e0zseg5c8ai">Industry Applications</a></li>
</ul>