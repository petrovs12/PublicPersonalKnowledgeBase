<h1 id="gradient-descent">Gradient Descent<a aria-hidden="true" class="anchor-heading icon-link" href="#gradient-descent"></a></h1>
<h1 id="introduction">Introduction<a aria-hidden="true" class="anchor-heading icon-link" href="#introduction"></a></h1>
<h1 id="stochastic-gradient-descent">Stochastic Gradient Descent<a aria-hidden="true" class="anchor-heading icon-link" href="#stochastic-gradient-descent"></a></h1>
<h1 id="minimatch-method">Minimatch Method<a aria-hidden="true" class="anchor-heading icon-link" href="#minimatch-method"></a></h1>
<p><a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">#minibatches (Private)</a></p>
<h1 id="adam-optimizer">Adam Optimizer<a aria-hidden="true" class="anchor-heading icon-link" href="#adam-optimizer"></a></h1>
<p><a href="/PublicPersonalKnowledgeBase/notes/icxjmJTn3DvuC07TCP9aj">Adam Optimizer</a></p>
<h1 id="second-order-optimizers">Second Order Optimizers<a aria-hidden="true" class="anchor-heading icon-link" href="#second-order-optimizers"></a></h1>
<p><a href="/PublicPersonalKnowledgeBase/notes/gEvZkRFpzl8TIsH53TlXn">Second Order Optimizers</a></p>
<p><a href="/PublicPersonalKnowledgeBase/notes/VLkBgpCqTp10y3UfjuCNe">Newton-Rhapson Method</a>
<a href="/PublicPersonalKnowledgeBase/notes/DomemlJ4RaKuAJ3oCsuTN">Stochastic Gradient Descent</a>
<a title="Private" href="https://wiki.dendron.so/notes/hfyvYGJZQiUwQaaxQO27q.html" target="_blank" class="private">Minimatch Method (Private)</a></p>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/PublicPersonalKnowledgeBase/notes/F4qDUpHVv34wdYvQu0h2O">Linear Algebra</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/4FIxH4rGwMUvVyCogNmHe">Optimization</a></li>
<li><a href="/PublicPersonalKnowledgeBase/notes/gz71FnqSIo2TfwivaD4Df">Optimizers In in Neural Networks</a></li>
</ul>